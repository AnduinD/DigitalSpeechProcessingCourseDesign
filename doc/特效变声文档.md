# 特效变声demo的小文档

AnduinD

把男的声音分别变成小孩的声音、女人的声音和老人的声音。

## 变声原理
### 发声机制

语音科学家将人类发声过程视作一个由声门源输送的气流经以声道、口、鼻腔组成的滤波器调制而成的。人类语音可分为有声语音和无声语音，前者是由声带振动激励的脉冲信号经声腔调制变成不同的音，它是人类语言中元音的基础，声带振动的频率称为基频。无声语音则是声带保持开启状态，禁止振动引发的。一般来说，由声门振动决定的基频跟说话人的性别特征有关，如下表，而无声语音则没有体现这个特征。说话人的个性化音色和语音的另外一个声学参数——共振峰频率的分布有关。儿童由于声道短，其共振峰频率高于成年人，成年女性的声道一般短于成年男性，所以女性的共振峰频率一般高于男性。

![在这里插入图片描述](E:\HUST学习\大三下\DAP\结课项目（特效变声）\voice_trans\doc\2021011208514639.png)

由上可知，在进行性别变声时，主要考虑**基频和共振峰频率**的变化。*当基频伸展，共振峰频率也同时伸展时，可由男声变成女声，女声变成童声；反之，基频收缩，共振峰频率也同时收缩时，则由童声变女声，女声变男声。*为了获得自然度、真实感较好的变声效果，基频和共振峰频率通常必须各自独立地伸缩变化如图。

![02](E:\HUST学习\大三下\DAP\结课项目（特效变声）\voice_trans\doc\02.png)

共振峰频率的改变是基于重采样实现的，从重采样原理知道，这也同时引发了基频的变化，为保证基频变化和共振峰频率变化的独立、互不相关，在基频移动时必须考虑抵消重采样带来的偏移，理论上只要基频检测足够精确，确实可以保证基频改变和共振峰频率改变间的互不相关。

## 系统设计

### 思路概述

如果想实现男声变女声，要改变语音信号的两个特征，分别是**音高和音色**：
1.音高是指说话人的音调，这个主要是由声带振动频率决定的，一般来说女声的音高是高于男声的，不过在信号处理里面我们更喜欢把音调说成**基频**，代表了一段语音的基础频率，也就是频谱图最下面那条；
2.音色是指说话人所具有的个人特征，这个在频谱上表现为**共振峰**，反应的是声道的特性。一般来说男生的音色会更加雄厚，而女生的则更加清爽一些。
下面就来介绍如何去改变这两个特征，主要分为基频追踪和信号重构两个部分。

### 语音特征提取（声纹识别）：

#### 预处理
一般来说变声算法都需要对原信号的基频进行计算，且基频的计算越精确，最后的变声效果也会越好。计算基频的方式有很多种，这里就不多介绍了，我用了自相关函数法来追踪基频，但是在计算之前需要对信号进行预处理，避免不必要的麻烦，具体如下：

+ a）去直流
  去除一帧信号的直流分量，尽可能减少直流分量带来的影响，下面的一些预处理都需要在去直流以后进行；
+ b）过零率检测
  语音分为清音和浊音，通常说话的爆破声都属于清音。考虑到清音段的基频难以辨别，故在在计算基频前通过过零率判断清浊，跳过清音段，把这一段作为噪声处理。
  那么如何去检测清音还是浊音，最简单有效的办法就是过零率检测，即检测信号在一帧内由正到负或者由负到正的次数，根据次数判断是清音还是浊音，一般来说清音的过零率会远大于浊音；
+ c）低通滤波
  由于语音信号中可能有高频噪声，或者是语音本身的共振峰，都会影响自相关函数的计算。当信号中有高频成分时，自相关函数会计算出多个波峰，而基频的计算是根据自相关函数的波峰位置确定的，波峰太多会提高后续基频筛选的难度，因此需要用低通滤波滤除。低通滤波的截止频率一般选取600Hz，这是考虑到有些人在说话时基频可能能够达到600Hz；
+ d）峰值检测
  除了排除清音以外，如果判断到当前帧是噪声，也需要跳过基频计算的步骤，这样可以避免错误的基频。我在算法中用了最简单的峰值检测来判断是否是噪声，这是基于一个假设：噪声段的信号强度一般小于语音段的信号强度，只要检测一帧内的最大峰值是否大于某个阈值，就可以判断这一帧是否是噪声。

#### 基音频率提取（pYIN算法）

+ 基于短时自相关法的基音周期估值（实验2）：修正的短时自相关+端点检测(具体实现看我实验报告就完事了)；
+ YIN算法实质上是修正的短时幅度差+谷点检测；

+ 长语音的基音提取改进：区分出清音帧、浊音帧、背景帧，仅在所有的浊音帧中进行提取，而后取平均。

#### 取得共振峰（倒谱法）

##### 共振峰估计的困难

+ 虚假共振峰的存在

+ 相邻共振峰频率离的比较近时难以区分

+ 高音调语音的提取有一定困难

和基音周期（基频）估计相同，目前没有完全准确的估计方法。

##### 共振峰估计的预处理

预加重：高频提升，以还原声门信号，去除口唇辐射的影响

+ 增加一个零点：抵消声门脉冲引起的高端频谱幅度下跌，使信号频谱变得平坦且各共振峰幅度相接近;语音中只剩下声道部分的影响，所提取的特征更加符合原声道的模型。
+ 会削减低频信息，使有些基频幅值变大时，通过预加重后降低基频对共振峰检测的干扰，有利于共振峰的检测；同时减少频谱的动态范围。

##### 共振峰估计方法

+ 倒谱法：对语音做倒谱处理可以把激励信号与声道响应分离出来，然后去掉激励信号，之后再做傅里叶变换，就可以得到声道响应的包络线，对包络上寻找极大值，就是相应的共振峰频率。

+ LPC法：由于通过线性预测我们可以求出一组预测系数，根据这组系数和全极点声道响应模型进行FFT变换，就可以得到声道传递函数的功率谱，再通过计算得到相应的极大值，就可以得到对应的共振峰频率
+ 内插法、HHT法等

##### 倒谱法共振峰估计的实现思路

取傅里叶对数频谱----倒谱加窗---取包络线---寻找极大值

<img src="E:\HUST学习\大三下\DAP\结课项目（特效变声）\voice_trans\doc\Figure_1.png" alt="Figure_1" style="zoom:50%;" />



### 语音变形处理

利用librosa库提供的音频处理API做基频的改变和共振峰的移动（pitch_shift,time_strech,...）

### 语音信号重建（语音合成）

#### WSOLA算法原理与实现

##### OLA音频变速算法

OLA音频变速算法即重叠叠加算法（Overlap-and-Add）。

![图片](E:\HUST学习\大三下\DAP\结课项目（特效变声）\voice_trans\doc\09bc836af578998e7f26f3e40d2bf3bd.png)

该算法有两阶段：分解与合成	

+ 分解阶段：语音首先分帧，帧长为N，假设帧移为Sa。

+ 合成阶段：分解出来的语音帧，以帧移为Ss的间隔重新合成起来，得到变速之后的音频。

由于分解与合成时的帧移参数不相同，故达到了变速而不引起频率失真的效果。通常定义规整因子$\alpha=S_a/S_s$ 来表征重建前后的音频变速的程度。

##### SOLA算法

SOLA算法（Synchronized Overlap-and-Add）的诞生是为了解决OLA算法中，由于移位导致的相位连续性缺失，从而引起基频断裂的问题。考虑在合成时引入动态的帧移：即首先引入一个离散的时间参数$k_m$，再通过$k_m$来确定相邻帧在间隔$S_s$附近重叠区的**最大相关**位置，最后把这个最大相关位置作为实际合成时的叠加位置。

![图片](E:\HUST学习\大三下\DAP\结课项目（特效变声）\voice_trans\doc\bb86647139372ec02e9cedc2846b9abf.png)

虽然SOLA算法一定程度上提高了规整信号平滑度，改善了基频断裂的问题，但由于动态的寻找互相关最高的同步点，导致了规整精度的降低，即叠加时没有严格按照$\alpha$的值去执行。

##### WSOLA算法

WSOLA波形相似性重叠相加算法（Waveform Similarity Based Overlap-Add）与SOLA算法的出发点相同，但不是通过移动叠加帧的位置，而是在原始帧的附近找到波形最相似的帧叠加到规整的位置（用互相关判定相似性），这样可以保证所有输出的帧移都满足规整因子的设置，同时最大程度地减少了基频断裂问题的影响。

![图片](E:\HUST学习\大三下\DAP\结课项目（特效变声）\voice_trans\doc\da07e2926207f57d1bd43100554fb091.png)

##### WSOLA代码实现思路

考虑输入参数为：待处理音频信号、原始采样率、规整因子、输出帧间隔（时间分辨率）。通过规整因子和输出帧移可以得到输入帧移，在确定首部帧的采样中心后，通过前后两帧头的位置，确定搜索范围，而后对下一个要插的帧位置，计算在搜索范围内的互相关序列，取最大值的位置为待插帧的首部，叠加完成后，再通过当前帧和后一阵的帧头位置确定下一帧的搜索范围，如此循环地计算互相关、插值叠加、计算新的搜索范围，最终完成变速后新序列的重建。(但这个算法不太能并行，我单核跑起来巨慢)

## 进一步探索（结合深度学习）

+ 基于GAN系列的语音变形（学习某个人的语音样本，去生成与其相仿的语音）

+ 基于声纹识别的哼唱识曲（语谱图+聚类+哈希查找）

+ 基于声纹识别和DNN的语音分类器（通过语谱图将语音特征提取并转换模态为图像，后利用深度神经网络做语音来源的分类（男人、女人、小孩、动物等））

+ 基于SVM的语音情感识别（本质还是分类，和上面一个差不多）



---

参考链接：

librosa文档：https://librosa.org/doc/latest/changelog.html

scipy文档：https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.detrend.html

Driedger J, Müller M. A review of time-scale modification of music signals[J]. Applied Sciences, 2016, 6(2): 57.

A. de Cheveigné and H. Kawahara, "YIN, a fundamental frequency estimator for speech and music", Journal of the Acoustical Society of America, 2002.

